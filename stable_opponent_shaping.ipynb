{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/qxcv/stable-opponent-shaping/blob/master/stable_opponent_shaping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pK54L-xijTot"
   },
   "source": [
    "Proof-of-concept implementation of Stable Opponent Shaping ([SOS](https://openreview.net/pdf?id=SyGjjsC5tQ)). Feel free to define any n-player game and compare SOS with other learning algorithms including Naive Learning (NL), [LOLA](https://arxiv.org/pdf/1709.04326.pdf), [LA](https://openreview.net/pdf?id=SyGjjsC5tQ), [CO](https://arxiv.org/pdf/1705.10461.pdf), [SGA](http://jmlr.csail.mit.edu/papers/volume20/19-008/19-008.pdf), [CGD](https://arxiv.org/pdf/1905.12103.pdf), [EG](https://arxiv.org/pdf/1906.05945.pdf) and [LSS](https://arxiv.org/pdf/1901.00838.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "WOCP-3QXijoq"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "L1rLMKEj0idp"
   },
   "outputs": [],
   "source": [
    "#@markdown Game definitions for matching pennies, iterated prisoner's\n",
    "#@markdown dilemma and tandem (define your own here).\n",
    "\n",
    "def matching_pennies():\n",
    "    dims = [1, 1]\n",
    "    payout_mat_1 = torch.Tensor([[1,-1],[-1,1]])\n",
    "    payout_mat_2 = -payout_mat_1\n",
    "    def Ls(th):\n",
    "        p_1, p_2 = torch.sigmoid(th[0]), torch.sigmoid(th[1])\n",
    "        x, y = torch.cat([p_1, 1-p_1]), torch.cat([p_2, 1-p_2])\n",
    "        L_1 = torch.matmul(torch.matmul(x, payout_mat_1), y)\n",
    "        L_2 = torch.matmul(torch.matmul(x, payout_mat_2), y)\n",
    "        return [L_1, L_2]\n",
    "    return dims, Ls\n",
    "\n",
    "def ipd(gamma=0.96):\n",
    "    dims = [5, 5]\n",
    "    payout_mat_1 = torch.Tensor([[-1,-3],[0,-2]])\n",
    "    payout_mat_2 = payout_mat_1.T\n",
    "    def Ls(th):\n",
    "        p_1_0 = torch.sigmoid(th[0][0:1])\n",
    "        p_2_0 = torch.sigmoid(th[1][0:1])\n",
    "        p = torch.cat([p_1_0*p_2_0, p_1_0*(1-p_2_0), (1-p_1_0)*p_2_0, (1-p_1_0)*(1-p_2_0)])\n",
    "        p_1 = torch.reshape(torch.sigmoid(th[0][1:5]), (4, 1))\n",
    "        p_2 = torch.reshape(torch.sigmoid(th[1][1:5]), (4, 1))\n",
    "        P = torch.cat([p_1*p_2, p_1*(1-p_2), (1-p_1)*p_2, (1-p_1)*(1-p_2)], dim=1)\n",
    "        M = -torch.matmul(p, torch.inverse(torch.eye(4)-gamma*P))\n",
    "        L_1 = torch.matmul(M, torch.reshape(payout_mat_1, (4, 1)))\n",
    "        L_2 = torch.matmul(M, torch.reshape(payout_mat_2, (4, 1)))\n",
    "        return [L_1, L_2]\n",
    "    return dims, Ls\n",
    "\n",
    "def tandem():\n",
    "    dims = [1, 1]\n",
    "    def Ls(th):\n",
    "        x, y = th\n",
    "        L_1 = (x+y)**2-2*x\n",
    "        L_2 = (x+y)**2-2*y\n",
    "        return [L_1, L_2]\n",
    "    return dims, Ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "QgmL9y6-6db_"
   },
   "outputs": [],
   "source": [
    "#@markdown Gradient computations for each algorithm.\n",
    "\n",
    "def init_th(dims, std):\n",
    "    th = []\n",
    "    for i in range(len(dims)):\n",
    "        if std > 0:\n",
    "            init = torch.nn.init.normal_(torch.empty(dims[i], requires_grad=True), std=std)\n",
    "        else:\n",
    "            init = torch.zeros(dims[i], requires_grad=True)\n",
    "        th.append(init)\n",
    "    return th\n",
    "\n",
    "def get_gradient(function, param):\n",
    "    grad = torch.autograd.grad(function, param, create_graph=True)[0]\n",
    "    return grad\n",
    "\n",
    "def get_hessian(th, grad_L, diag=True, off_diag=True):\n",
    "    n = len(th)\n",
    "    H = []\n",
    "    for i in range(n):\n",
    "        row_block = []\n",
    "        for j in range(n):\n",
    "            if (i == j and diag) or (i != j and off_diag):\n",
    "                block = [torch.unsqueeze(get_gradient(grad_L[i][i][k], th[j]), dim=0) \n",
    "                         for k in range(len(th[i]))]\n",
    "                row_block.append(torch.cat(block, dim=0))\n",
    "            else:\n",
    "                row_block.append(torch.zeros(len(th[i]), len(th[j])))\n",
    "        H.append(torch.cat(row_block, dim=1))\n",
    "    return torch.cat(H, dim=0)\n",
    "\n",
    "def update_th(th, Ls, alpha, algo, a=0.5, b=0.1, gam=1, ep=0.1, lss_lam=0.1):\n",
    "    n = len(th)\n",
    "    losses = Ls(th)\n",
    "\n",
    "    # Compute gradients\n",
    "    grad_L = [[get_gradient(losses[j], th[i]) for j in range(n)] for i in range(n)]\n",
    "    if algo == 'la':\n",
    "        terms = [sum([torch.dot(grad_L[j][i], grad_L[j][j].detach())\n",
    "                 for j in range(n) if j != i]) for i in range(n)]\n",
    "        grads = [grad_L[i][i]-alpha*get_gradient(terms[i], th[i]) for i in range(n)]\n",
    "    elif algo == 'lola':\n",
    "        terms = [sum([torch.dot(grad_L[j][i], grad_L[j][j])\n",
    "                 for j in range(n) if j != i]) for i in range(n)]\n",
    "        grads = [grad_L[i][i]-alpha*get_gradient(terms[i], th[i]) for i in range(n)]\n",
    "    elif algo == 'sos':\n",
    "        terms = [sum([torch.dot(grad_L[j][i], grad_L[j][j].detach())\n",
    "                 for j in range(n) if j != i]) for i in range(n)]\n",
    "        xi_0 = [grad_L[i][i]-alpha*get_gradient(terms[i], th[i]) for i in range(n)]\n",
    "        chi = [get_gradient(sum([torch.dot(grad_L[j][i].detach(), grad_L[j][j])\n",
    "                 for j in range(n) if j != i]), th[i]) for i in range(n)]\n",
    "        # Compute p\n",
    "        dot = torch.dot(-alpha*torch.cat(chi), torch.cat(xi_0))\n",
    "        p1 = 1 if dot >= 0 else min(1, -a*torch.norm(torch.cat(xi_0))**2/dot)\n",
    "        xi = torch.cat([grad_L[i][i] for i in range(n)])\n",
    "        xi_norm = torch.norm(xi)\n",
    "        p2 = xi_norm**2 if xi_norm < b else 1\n",
    "        p = min(p1, p2)\n",
    "        grads = [xi_0[i]-p*alpha*chi[i] for i in range(n)]\n",
    "    elif algo == 'sga':\n",
    "        xi = torch.cat([grad_L[i][i] for i in range(n)])\n",
    "        ham = torch.dot(xi, xi.detach())\n",
    "        H_t_xi = [get_gradient(ham, th[i]) for i in range(n)]\n",
    "        H_xi = [get_gradient(sum([torch.dot(grad_L[j][i], grad_L[j][j].detach())\n",
    "                for j in range(n)]), th[i]) for i in range(n)]\n",
    "        A_t_xi = [H_t_xi[i]/2-H_xi[i]/2 for i in range(n)]\n",
    "        # Compute lambda (sga with alignment)\n",
    "        dot_xi = torch.dot(xi, torch.cat(H_t_xi))\n",
    "        dot_A = torch.dot(torch.cat(A_t_xi), torch.cat(H_t_xi))\n",
    "        d = sum([len(th[i]) for i in range(n)])\n",
    "        lam = torch.sign(dot_xi*dot_A/d+ep)\n",
    "        grads = [grad_L[i][i]+lam*A_t_xi[i] for i in range(n)]\n",
    "    elif algo == 'co':\n",
    "        xi = torch.cat([grad_L[i][i] for i in range(n)])\n",
    "        ham = torch.dot(xi, xi.detach())\n",
    "        grads = [grad_L[i][i]+gam*get_gradient(ham, th[i]) for i in range(n)]\n",
    "    elif algo == 'eg':\n",
    "        th_eg = [th[i]-alpha*get_gradient(losses[i], th[i]) for i in range(n)]\n",
    "        losses_eg = Ls(th_eg)\n",
    "        grads = [get_gradient(losses_eg[i], th_eg[i]) for i in range(n)]\n",
    "    elif algo == 'cgd': # Slow implementation (matrix inversion)\n",
    "        dims = [len(th[i]) for i in range(n)]\n",
    "        xi = torch.cat([grad_L[i][i] for i in range(n)])\n",
    "        H_o = get_hessian(th, grad_L, diag=False)\n",
    "        grad = torch.matmul(torch.inverse(torch.eye(sum(dims))+alpha*H_o), xi)\n",
    "        grads = [grad[sum(dims[:i]):sum(dims[:i+1])] for i in range(n)]\n",
    "    elif algo == 'lss': # Slow implementation (matrix inversion)\n",
    "        dims = [len(th[i]) for i in range(n)]\n",
    "        xi = torch.cat([grad_L[i][i] for i in range(n)])\n",
    "        H = get_hessian(th, grad_L)\n",
    "        if torch.det(H) == 0:\n",
    "            inv = torch.inverse(torch.matmul(H.T, H)+lss_lam*torch.eye(sum(dims)))\n",
    "            H_inv = torch.matmul(inv, H.T)\n",
    "        else:\n",
    "            H_inv = torch.inverse(H)\n",
    "        grad = torch.matmul(torch.eye(sum(dims))+torch.matmul(H.T, H_inv), xi)/2\n",
    "        grads = [grad[sum(dims[:i]):sum(dims[:i+1])] for i in range(n)]\n",
    "    else: # Naive Learning\n",
    "        grads = [grad_L[i][i] for i in range(n)]\n",
    "\n",
    "    # Update theta\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            th[i] -= alpha*grads[i]\n",
    "    return th, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "QgmL9y6-6db_"
   },
   "outputs": [],
   "source": [
    "#@markdown Noisy gradient calculations\n",
    "\n",
    "def noisify_th(th, i, sigma):\n",
    "    \"\"\"Add noise to each entry of th EXCEPT the ith one.\"\"\"\n",
    "    return [\n",
    "        t if j == i else t + sigma * torch.randn_like(t)\n",
    "        for j, t in enumerate(th)\n",
    "    ]\n",
    "\n",
    "def update_th_noisy(th, Ls, alpha, algo, a=0.5, b=0.1, gam=1, ep=0.1, lss_lam=0.1, sigma=0.0):\n",
    "    n = len(th)\n",
    "    true_th = th\n",
    "    true_grads = [None] * n\n",
    "\n",
    "    # Update theta\n",
    "    for i in range(n):\n",
    "        th = true_th\n",
    "\n",
    "        if sigma:\n",
    "            # add noise to opponents' parameters, if necessary\n",
    "            th = noisify_th(th, i, sigma)\n",
    "\n",
    "        losses = Ls(th)\n",
    "\n",
    "        # Compute gradients\n",
    "        grad_L = [[get_gradient(losses[j], th[i]) for j in range(n)] for i in range(n)]\n",
    "        if algo == 'la':\n",
    "            terms = [sum([torch.dot(grad_L[j][i], grad_L[j][j].detach())\n",
    "                     for j in range(n) if j != i]) for i in range(n)]\n",
    "            grads = [grad_L[i][i]-alpha*get_gradient(terms[i], th[i]) for i in range(n)]\n",
    "        elif algo == 'lola':\n",
    "            terms = [sum([torch.dot(grad_L[j][i], grad_L[j][j])\n",
    "                     for j in range(n) if j != i]) for i in range(n)]\n",
    "            grads = [grad_L[i][i]-alpha*get_gradient(terms[i], th[i]) for i in range(n)]\n",
    "        elif algo == 'sos':\n",
    "            terms = [sum([torch.dot(grad_L[j][i], grad_L[j][j].detach())\n",
    "                     for j in range(n) if j != i]) for i in range(n)]\n",
    "            xi_0 = [grad_L[i][i]-alpha*get_gradient(terms[i], th[i]) for i in range(n)]\n",
    "            chi = [get_gradient(sum([torch.dot(grad_L[j][i].detach(), grad_L[j][j])\n",
    "                     for j in range(n) if j != i]), th[i]) for i in range(n)]\n",
    "            # Compute p\n",
    "            dot = torch.dot(-alpha*torch.cat(chi), torch.cat(xi_0))\n",
    "            p1 = 1 if dot >= 0 else min(1, -a*torch.norm(torch.cat(xi_0))**2/dot)\n",
    "            xi = torch.cat([grad_L[i][i] for i in range(n)])\n",
    "            xi_norm = torch.norm(xi)\n",
    "            p2 = xi_norm**2 if xi_norm < b else 1\n",
    "            p = min(p1, p2)\n",
    "            grads = [xi_0[i]-p*alpha*chi[i] for i in range(n)]\n",
    "        elif algo == 'sga':\n",
    "            xi = torch.cat([grad_L[i][i] for i in range(n)])\n",
    "            ham = torch.dot(xi, xi.detach())\n",
    "            H_t_xi = [get_gradient(ham, th[i]) for i in range(n)]\n",
    "            H_xi = [get_gradient(sum([torch.dot(grad_L[j][i], grad_L[j][j].detach())\n",
    "                    for j in range(n)]), th[i]) for i in range(n)]\n",
    "            A_t_xi = [H_t_xi[i]/2-H_xi[i]/2 for i in range(n)]\n",
    "            # Compute lambda (sga with alignment)\n",
    "            dot_xi = torch.dot(xi, torch.cat(H_t_xi))\n",
    "            dot_A = torch.dot(torch.cat(A_t_xi), torch.cat(H_t_xi))\n",
    "            d = sum([len(th[i]) for i in range(n)])\n",
    "            lam = torch.sign(dot_xi*dot_A/d+ep)\n",
    "            grads = [grad_L[i][i]+lam*A_t_xi[i] for i in range(n)]\n",
    "        elif algo == 'co':\n",
    "            xi = torch.cat([grad_L[i][i] for i in range(n)])\n",
    "            ham = torch.dot(xi, xi.detach())\n",
    "            grads = [grad_L[i][i]+gam*get_gradient(ham, th[i]) for i in range(n)]\n",
    "        elif algo == 'eg':\n",
    "            th_eg = [th[i]-alpha*get_gradient(losses[i], th[i]) for i in range(n)]\n",
    "            losses_eg = Ls(th_eg)\n",
    "            grads = [get_gradient(losses_eg[i], th_eg[i]) for i in range(n)]\n",
    "        elif algo == 'cgd': # Slow implementation (matrix inversion)\n",
    "            dims = [len(th[i]) for i in range(n)]\n",
    "            xi = torch.cat([grad_L[i][i] for i in range(n)])\n",
    "            H_o = get_hessian(th, grad_L, diag=False)\n",
    "            grad = torch.matmul(torch.inverse(torch.eye(sum(dims))+alpha*H_o), xi)\n",
    "            grads = [grad[sum(dims[:i]):sum(dims[:i+1])] for i in range(n)]\n",
    "        elif algo == 'lss': # Slow implementation (matrix inversion)\n",
    "            dims = [len(th[i]) for i in range(n)]\n",
    "            xi = torch.cat([grad_L[i][i] for i in range(n)])\n",
    "            H = get_hessian(th, grad_L)\n",
    "            if torch.det(H) == 0:\n",
    "                inv = torch.inverse(torch.matmul(H.T, H)+lss_lam*torch.eye(sum(dims)))\n",
    "                H_inv = torch.matmul(inv, H.T)\n",
    "            else:\n",
    "                H_inv = torch.inverse(H)\n",
    "            grad = torch.matmul(torch.eye(sum(dims))+torch.matmul(H.T, H_inv), xi)/2\n",
    "            grads = [grad[sum(dims[:i]):sum(dims[:i+1])] for i in range(n)]\n",
    "        else: # Naive Learning\n",
    "            grads = [grad_L[i][i] for i in range(n)]\n",
    "        true_grads[i] = grads[i]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            th[i] -= alpha*true_grads[i]\n",
    "    return th, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "colab_type": "code",
    "id": "lUfvYbbZ9dhi",
    "outputId": "53047381-c25b-49b9-b795-ca09a1d44d72"
   },
   "outputs": [],
   "source": [
    "'''Run SOS on the game of your choosing below'''\n",
    "\n",
    "# Select game\n",
    "# dims, Ls = matching_pennies()\n",
    "dims, Ls = ipd()\n",
    "\n",
    "# Set num_epochs, learning rate and learning algo\n",
    "num_epochs = 100\n",
    "alpha = 1\n",
    "algo = 'lola'  #('sos', 'lola', 'la', 'sga', 'co', 'eg', 'cgd', 'lss' or 'nl')\n",
    "\n",
    "# Initialise theta ~ Normal(0, std)\n",
    "std = 1\n",
    "th = init_th(dims, std)\n",
    "\n",
    "# Run\n",
    "losses_out = np.zeros((num_epochs, len(th)))\n",
    "th_out = []\n",
    "for k in range(num_epochs):\n",
    "    # th, losses = update_th(th, Ls, alpha, algo)\n",
    "    th, losses = update_th_noisy(th, Ls, alpha, algo)\n",
    "    th_out.append([th[i].data.numpy() for i in range(len(th))])\n",
    "    losses_out[k] = [loss.data.numpy() for loss in losses]\n",
    "plt.plot(losses_out)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "A16rzMaE5A_P",
    "outputId": "390cab07-21d6-4584-9053-dfce1ab64518"
   },
   "outputs": [],
   "source": [
    "'''Iterated Prisoner's Dilemma - SOS/LOLA vs LA/CO/SGA/EG/CGD/LSS/NL'''\n",
    "\n",
    "gamma = 0.96\n",
    "sigma = 0.0\n",
    "dims, Ls = ipd(gamma)\n",
    "\n",
    "num_runs = 50\n",
    "num_epochs = 100\n",
    "alpha = 1\n",
    "std = 1\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "for algo in ['sos', 'lola', 'la', 'co', 'sga', 'eg', 'cgd', 'lss', 'nl']:\n",
    "    losses_out = np.zeros((num_runs, num_epochs))\n",
    "    for i in range(num_runs):\n",
    "        th = init_th(dims, std)\n",
    "        for k in range(num_epochs):\n",
    "            # th, losses = update_th(th, Ls, alpha, algo)\n",
    "            th, losses = update_th_noisy(th, Ls, alpha, algo, sigma=sigma)\n",
    "            losses_out[i, k] = (1-gamma)*losses[0].data.numpy()\n",
    "    mean = np.mean(losses_out, axis=0)\n",
    "    dev = np.std(losses_out, axis=0)\n",
    "    plt.plot(np.arange(num_epochs), mean)\n",
    "    plt.fill_between(np.arange(num_epochs), mean-dev, mean+dev, alpha=0.08)\n",
    "\n",
    "plt.title(fr'IPD Results ($\\sigma = {sigma}$)')\n",
    "plt.xlabel('Learning Step')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.legend(['SOS', 'LOLA', 'LA', 'CO', 'SGA', 'EG', 'CGD', 'LSS', 'NL'], loc='upper left', frameon=True, framealpha=1, ncol=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "8m-dBrG4nJ52",
    "outputId": "2d49abb8-7843-45f3-de48-951f28d9a3b9"
   },
   "outputs": [],
   "source": [
    "'''Tandem Game - SOS vs LOLA'''\n",
    "\n",
    "dims, Ls = tandem()\n",
    "\n",
    "num_runs = 100\n",
    "num_epochs = 30\n",
    "alpha = 0.1\n",
    "std = 0.1\n",
    "\n",
    "for algo in ['sos', 'lola']:\n",
    "    losses_out = np.zeros((num_runs, num_epochs))\n",
    "    for i in range(num_runs):\n",
    "        th = init_th(dims, std)\n",
    "        for k in range(num_epochs):\n",
    "            th, losses = update_th(th, Ls, alpha, algo)\n",
    "            losses_out[i, k] = losses[0].data.numpy()\n",
    "\n",
    "    mean = np.mean(losses_out, axis=0)\n",
    "    dev = np.std(losses_out, axis=0)\n",
    "    plt.plot(np.arange(num_epochs), mean)\n",
    "    plt.fill_between(np.arange(num_epochs), mean-dev, mean+dev, alpha=0.1)\n",
    "\n",
    "plt.title('Tandem Game Results')\n",
    "plt.xlabel('Learning Step')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.legend(['SOS', 'LOLA'], loc='upper left', frameon=True, framealpha=1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "stable-opponent-shaping.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
